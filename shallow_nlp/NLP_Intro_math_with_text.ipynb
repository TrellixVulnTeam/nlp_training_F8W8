{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is natural language processing (NLP)?\n",
    "Natural language processing is an area of research in computer science and artificial intelligence (AI) concerned with processing natural languages such as English or Mandarin. This processing generally involves\n",
    "translating natural language into data (numbers) that a computer can use to\n",
    "learn about the world. And this understanding of the world is sometimes used\n",
    "to generate natural language text that reflects that understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP Application](img/nlpa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building your vocabulary with a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, tokenization is a particular kind of document segmentation. Segmentation\n",
    "breaks up text into smaller chunks or segments, with more focused information content. Segmentation can include breaking a document into paragraphs, paragraphs\n",
    "into sentences, sentences into phrases, or phrases into tokens (usually words) and\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization** is the first step in an NLP pipeline, so it can have a big impact on the\n",
    "rest of your pipeline. A tokenizer breaks unstructured data, natural language text,\n",
    "into chunks of information that can be counted as discrete elements. These counts of\n",
    "token occurrences in a document can be used directly as a vector representing that\n",
    "document. This immediately turns an unstructured string (text document) into a\n",
    "numerical data structure suitable for machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The simplest way to tokenize a sentence is to use whitespace within a string as the\n",
    "“delimiter” of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Ftech is a AI-oriented company Apple is a company which sales apple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ftech',\n",
       " 'is',\n",
       " 'a',\n",
       " 'AI-oriented',\n",
       " 'company',\n",
       " 'Apple',\n",
       " 'is',\n",
       " 'a',\n",
       " 'company',\n",
       " 'which',\n",
       " 'sales',\n",
       " 'apple']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a bit more Python, you can create a numerical\n",
    "vector representation for each word. These vectors are called *one-hot vectors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sequence = str.split(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(token_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI-oriented',\n",
       " 'Apple',\n",
       " 'Ftech',\n",
       " 'a',\n",
       " 'apple',\n",
       " 'company',\n",
       " 'is',\n",
       " 'sales',\n",
       " 'which']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "onehot_vectors = np.zeros((num_tokens,vocab_size), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(token_sequence):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have trouble quickly reading all those ones and zeros, you’re not alone. **Pandas**\n",
    "DataFrames can help make this a little easier on the eyes and more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AI-oriented</th>\n",
       "      <th>Apple</th>\n",
       "      <th>Ftech</th>\n",
       "      <th>a</th>\n",
       "      <th>apple</th>\n",
       "      <th>company</th>\n",
       "      <th>is</th>\n",
       "      <th>sales</th>\n",
       "      <th>which</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AI-oriented  Apple  Ftech  a  apple  company  is  sales  which\n",
       "0             0      0      1  0      0        0   0      0      0\n",
       "1             0      0      0  0      0        0   1      0      0\n",
       "2             0      0      0  1      0        0   0      0      0\n",
       "3             1      0      0  0      0        0   0      0      0\n",
       "4             0      0      0  0      0        1   0      0      0\n",
       "5             0      1      0  0      0        0   0      0      0\n",
       "6             0      0      0  0      0        0   1      0      0\n",
       "7             0      0      0  1      0        0   0      0      0\n",
       "8             0      0      0  0      0        1   0      0      0\n",
       "9             0      0      0  0      0        0   0      0      1\n",
       "10            0      0      0  0      0        0   0      1      0\n",
       "11            0      0      0  0      1        0   0      0      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(onehot_vectors, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice feature of this vector representation of words and tabular representation\n",
    "of documents is that no information is lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s assume you\n",
    "have a million tokens in your NLP pipeline vocabulary. \n",
    "And let’s say you have a meager 3,000 books with 3,500 sentences each and 15 words per sentence—reasonable\n",
    "averages for short books => how much memory do we need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157500000000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rows = 3000 * 3500 * 15\n",
    "num_bytes = num_rows * 1000000\n",
    "num_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157500.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_bytes / 1e9 # gigabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " _ / 1000 #terabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP Application](img/wic.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  word frequency vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you summed all these one-hot vectors together, rather than “replaying” them\n",
    "one at a time, you’d get a bag-of-words vector. This is also called a word frequency vector, because it only counts the frequency of words, not their order. You could use this\n",
    "single vector to represent the whole document or sentence in a single, reasonablelength vector. It would only be as long as your vocabulary size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s what your single text document,\n",
    "looks like as a binary bag-of-words vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bow = {}\n",
    "for token in sentence.split():\n",
    "    sentence_bow[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ftech': 1,\n",
       " 'is': 1,\n",
       " 'a': 1,\n",
       " 'AI-oriented': 1,\n",
       " 'company': 1,\n",
       " 'Apple': 1,\n",
       " 'which': 1,\n",
       " 'sales': 1,\n",
       " 'apple': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s add a few more texts to your corpus to see how a DataFrame stacks up. A\n",
    "DataFrame indexes both the columns (documents) and rows (words) so it can be an\n",
    "“inverse index” for document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"Thomas Jefferson began building Monticello at the age of 26.\\n\"\n",
    "sentences += \"Construction was done mostly by local masons and carpenters.\\n\"\n",
    "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
    "sentences += \"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\n",
    "corpus = {}\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "     corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent0': {'Thomas': 1,\n",
       "  'Jefferson': 1,\n",
       "  'began': 1,\n",
       "  'building': 1,\n",
       "  'Monticello': 1,\n",
       "  'at': 1,\n",
       "  'the': 1,\n",
       "  'age': 1,\n",
       "  'of': 1,\n",
       "  '26.': 1},\n",
       " 'sent1': {'Construction': 1,\n",
       "  'was': 1,\n",
       "  'done': 1,\n",
       "  'mostly': 1,\n",
       "  'by': 1,\n",
       "  'local': 1,\n",
       "  'masons': 1,\n",
       "  'and': 1,\n",
       "  'carpenters.': 1},\n",
       " 'sent2': {'He': 1,\n",
       "  'moved': 1,\n",
       "  'into': 1,\n",
       "  'the': 1,\n",
       "  'South': 1,\n",
       "  'Pavilion': 1,\n",
       "  'in': 1,\n",
       "  '1770.': 1},\n",
       " 'sent3': {'Turning': 1,\n",
       "  'Monticello': 1,\n",
       "  'into': 1,\n",
       "  'a': 1,\n",
       "  'neoclassical': 1,\n",
       "  'masterpiece': 1,\n",
       "  'was': 1,\n",
       "  \"Jefferson's\": 1,\n",
       "  'obsession.': 1}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1770.</th>\n",
       "      <th>26.</th>\n",
       "      <th>Construction</th>\n",
       "      <th>He</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>Jefferson's</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>Pavilion</th>\n",
       "      <th>South</th>\n",
       "      <th>Thomas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1770.  26.  Construction  He  Jefferson  Jefferson's  Monticello  \\\n",
       "sent0      0    1             0   0          1            0           1   \n",
       "sent1      0    0             1   0          0            0           0   \n",
       "sent2      1    0             0   1          0            0           0   \n",
       "sent3      0    0             0   0          0            1           1   \n",
       "\n",
       "       Pavilion  South  Thomas  \n",
       "sent0         0      0       1  \n",
       "sent1         0      0       0  \n",
       "sent2         1      1       0  \n",
       "sent3         0      0       0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df[df.columns[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dot product\n",
    "The dot product is also called the inner product because the “inner” dimension of\n",
    "the two vectors (the number of elements in each vector) or matrices (the rows of the\n",
    "first matrix and the columns of the second matrix) must be the same, because that’s\n",
    "where the products happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = pd.np.array([1, 2, 3])\n",
    "v2 = pd.np.array([2, 3, 4])\n",
    "v1.dot(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Measuring bag-of-words overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can measure the bag of words overlap for two vectors, we can get a good estimate\n",
    "of how similar they are in the words they use. And this is a good estimate of how similar they are in meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.T\n",
    "df.sent0.dot(df.sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sent0.dot(df.sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sent0.dot(df.sent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this you can tell that one word was used in both sent0 and sent2. Likewise one\n",
    "of the words in your vocabulary was used in both sent0 and sent3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A token improvement\n",
    "In some situations, other characters besides spaces are used to separate words in a sentence. You need\n",
    "your tokenizer to split a sentence not just on whitespace, but also on punctuation such\n",
    "as commas, periods, quotes, semicolons, and even hyphens (dashes). In some cases\n",
    "you want these punctuation marks to be treated like words, as independent tokens. In\n",
    "other cases you may want to ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"    \n",
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several Python libraries implement tokenizers, each with its own advantages and\n",
    "disadvantages:\n",
    "* spaCy—Accurate , flexible, fast, Python\n",
    "* Stanford CoreNLP—More accurate, less flexible, fast, depends on Java 8\n",
    "* NLTK—Standard used by many NLP contests and comparisons, popular, Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending your vocabulary with n-grams\n",
    "An n-gram is a sequence containing up to n elements that have been extracted from a\n",
    "sequence of those elements, usually a string. In general the “elements” of an n-gram\n",
    "can be characters, syllables, words, or even symbols like “A,” “T,” “G,” and “C” used to\n",
    "represent a DNA sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why bother with n-grams? As you saw earlier, when a sequence of tokens is vectorized into a bag-of-words vector, it loses a lot of the meaning inherent in the order of\n",
    "those words. By extending your concept of a token to include multiword tokens,\n",
    "n-grams, your NLP pipeline can retain much of the meaning inherent in the order of\n",
    "words in your statements. For example, the meaning-inverting word “not” will remain\n",
    "attached to its neighboring words, where it belongs. Without n-gram tokenization, it\n",
    "would be free floating. Its meaning would be associated with the entire sentence or\n",
    "document rather than its neighboring words. The 2-gram “was not” retains much\n",
    "more of the meaning of the individual words “not” and “was” than those 1-grams\n",
    "alone in a bag-of-words vector. A bit of the context of a word is retained when you tie it\n",
    "to its neighbor(s) in your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thomas', 'Jefferson'),\n",
       " ('Jefferson', 'began'),\n",
       " ('began', 'building'),\n",
       " ('building', 'Monticello'),\n",
       " ('Monticello', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'age'),\n",
       " ('age', 'of'),\n",
       " ('of', '26'),\n",
       " ('26', '')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP WORDS\n",
    "Stop words are common words in any language that occur with a high frequency but\n",
    "carry much less substantive information about the meaning of a phrase. Examples of\n",
    "some common stop words include\n",
    "* a, an\n",
    "* the, this\n",
    "* and, or\n",
    "* of, on\n",
    "Historically, stop words have been excluded from NLP pipelines in order to reduce\n",
    "the computational effort to extract information from a text. Even though the words\n",
    "themselves carry little information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/andy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing your vocabulary\n",
    "#### CASE FOLDING\n",
    "In Python, you can easily normalize the capitalization of your tokens with a list\n",
    "comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'visitor', 'center']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['House', 'Visitor', 'Center']\n",
    "normalized_tokens = [x.lower() for x in tokens]\n",
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEMMING\n",
    "Another common vocabulary normalization technique is to eliminate the small meaning differences of pluralization or possessive endings of words, or even various verb\n",
    "forms. This normalization, identifying a common stem among various forms of a\n",
    "word, is called stemming. For example, the words housing and houses share the same\n",
    "stem, house. Stemming removes suffixes from words in an attempt to combine words\n",
    "with similar meanings together under their common stem. A stem isn’t required to be\n",
    "a properly spelled word, but merely a token, or label, representing several possible\n",
    "spellings of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish washer wash dish'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEMMATIZATION\n",
    "If you have access to information about connections between the meanings of various\n",
    "words, you might be able to associate several words together even if their spelling is\n",
    "quite different. This more extensive normalization down to the semantic root of a\n",
    "word—its lemma—is called lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment\n",
    "Whether you use raw single-word tokens, n-grams, stems, or lemmas in your NLP pipeline, each of those tokens contains some information. An important part of this information is the word’s sentiment—the overall feeling or emotion that the word invokes.\n",
    "This sentiment analysis—measuring the sentiment of phrases or chunks of text—is a\n",
    "common application of NLP. In many companies it’s the main thing an NLP engineer\n",
    "is asked to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words\n",
    "In the previous section, you created your first vector space model of a text. You used\n",
    "one-hot encoding of each word and then combined all those vectors with a binary OR\n",
    "(or clipped sum) to create a vector representation of a text. And this binary bag-ofwords vector makes a great index for document retrieval when loaded into a data\n",
    "structure such as a Pandas DataFrame. You then looked at an even more useful vector representation that counts the\n",
    "number of occurrences, or frequency, of each word in the given text\n",
    "Let’s look at an example where counting occurrences of words is useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your simple list, you want to get unique words from the document and their\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For short documents like this one, the unordered bag of words still contains a lot of\n",
    "information about the original intent of the sentence. And the information in a bag of\n",
    "words is sufficient to do some powerful things such as detect spam, compute sentiment (positivity, happiness, and so on), and even detect subtle intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the number of times a word occurs in a given document is called the *term\n",
    "frequency*, commonly abbreviated **TF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s calculate the term frequency of “harry” from the Counter object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18181818181818182"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_harry_appears = bag_of_words['harry']\n",
    "num_unique_words = len(bag_of_words)\n",
    "tf = times_harry_appears / num_unique_words\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s pause for a second and look a little deeper at normalized term frequency, a\n",
    "phrase (and calculation) we use often throughout this book. It’s the word count tempered by how long the document is. But why “temper” it all? Let’s say you find the\n",
    "word “dog” 3 times in document A and 100 times in document B. Clearly “dog” is way\n",
    "more important to document B. But wait. Let’s say you find out document A is a\n",
    "30-word email to a veterinarian and document B is War & Peace (approx 580,000\n",
    "words!). Your first analysis was straight-up backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's jump to a bigger corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pt36/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()    \n",
    "from nlpia.data.loaders import kite_text\n",
    "tokens = tokenizer.tokenize(kite_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 26),\n",
       " ('a', 20),\n",
       " ('kite', 16),\n",
       " (',', 15),\n",
       " ('and', 10),\n",
       " ('of', 10),\n",
       " ('kites', 8),\n",
       " ('is', 7),\n",
       " ('in', 7),\n",
       " ('or', 6)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts = Counter(tokens)\n",
    "token_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s not likely that this Wikipedia article is about the articles “the” and “a,” nor the conjunction “and” and the other\n",
    "stop words. So let’s ditch them for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kite', 16),\n",
       " (',', 15),\n",
       " ('kites', 8),\n",
       " ('wing', 5),\n",
       " ('lift', 4),\n",
       " ('may', 4),\n",
       " ('also', 3),\n",
       " ('kiting', 3),\n",
       " ('flown', 3),\n",
       " ('tethered', 2)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)\n",
    "kite_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve transformed your text into numbers on a basic level. But you’ve still just stored\n",
    "them in a dictionary, so you’ve taken one step out of the text-based world and into the\n",
    "realm of mathematics. Next you’ll go ahead and jump in all the way. Instead of\n",
    "describing a document in terms of a frequency dictionary, you’ll make a vector of\n",
    "those word counts. In Python, this will be a list, but in general it’s an ordered collection or array. You can do this quickly with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07207207207207207, 0.06756756756756757, 0.036036036036036036]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in kite_counts.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "document_vector[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You had one “document” already—\n",
    "let’s round out the corpus with a couple more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vector spaces\n",
    "Vectors are the primary building blocks of linear algebra, or vector algebra. They’re\n",
    "an ordered list of numbers, or coordinates, in a vector space. They describe a location\n",
    "or position in that space. Or they can be used to identify a particular direction and\n",
    "magnitude or distance in that space. A space is the collection of all possible vectors that\n",
    "could appear in that space. So a vector with two values would lie in a 2D vector space,\n",
    "a vector with three values in 3D vector space, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a natural language document vector space, the dimensionality of your vector\n",
    "space is the count of the number of distinct words that appear in the entire corpus.\n",
    "For TF (and TF-IDF to come)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two vectors are “similar” if they share similar direction. They might have similar\n",
    "magnitude (length), which would mean that the word count (term frequency) vectors\n",
    "are for documents of about the same length. But do you care about document length\n",
    "in your similarity estimate for vector representations of words in documents? Probably\n",
    "not. You’d like your estimate of document similarity to find use of the same words\n",
    "about the same number of times in similar proportions. This accurate estimate would\n",
    "give you confidence that the documents they represent are probably talking about\n",
    "similar things.\n",
    " Cosine similarity is merely the cosine of the angle between two vectors (theta),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hehe](img/cs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hehe](img/2dspace.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\" Let's convert our dictionaries to lists for easier matching.\"\"\"\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hehe](img/tfidf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16 0.   0.48 0.21 0.21 0.   0.25 0.21 0.   0.   0.   0.21 0.   0.64\n",
      "  0.21 0.21]\n",
      " [0.37 0.   0.37 0.   0.   0.37 0.29 0.   0.37 0.37 0.   0.   0.49 0.\n",
      "  0.   0.  ]\n",
      " [0.   0.75 0.   0.   0.   0.29 0.22 0.   0.29 0.29 0.38 0.   0.   0.\n",
      "  0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = docs\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "print(model.todense().round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
